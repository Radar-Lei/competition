{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing from imputed data\n",
    "The .npy have 97 days of data where the first 90 days are for training, and the remaining is for testing.\n",
    "Notice that we need to use data in .npy to only fill the NaN values in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './dataset/competition/missing_data_impu/'\n",
    "flow_impu = np.load('./dataset/competition/missing_data_impu/flow-5min_imputed.npy')\n",
    "speed_impu = np.load('./dataset/competition/missing_data_impu/speed-5min_imputed.npy')\n",
    "\n",
    "flow_train_impu = np.round(flow_impu[:,:,:90], decimals=0)\n",
    "K,D,L = flow_train_impu.shape\n",
    "flow_train_impu = flow_train_impu.reshape(K,D*L).transpose(1,0)\n",
    "\n",
    "flow_test_impu = np.round(flow_impu[:,:,90:], decimals=0)\n",
    "K,D,L = flow_test_impu.shape\n",
    "flow_test_impu = flow_test_impu.reshape(K,D*L).transpose(1,0)\n",
    "\n",
    "speed_train_impu = np.round(speed_impu[:,:,:90], decimals=2)\n",
    "K,D,L = speed_train_impu.shape\n",
    "speed_train_impu = speed_train_impu.reshape(K,D*L).transpose(1,0)\n",
    "\n",
    "speed_test_impu = np.round(speed_impu[:,:,90:], decimals=2)\n",
    "K,D,L = speed_test_impu.shape\n",
    "speed_test_impu = speed_test_impu.reshape(K,D*L).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train_impu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './dataset/competition/train-5min'\n",
    "def load_train_raw(root_path):\n",
    "    datetime_5min_train_ids = pd.date_range(start='2023-04-02', end='2023-07-01', freq='5min', inclusive='both')[:-1]\n",
    "\n",
    "    # get a list of all the csv files in the directory\n",
    "     # './dataset/train-30s'    './dataset/train-5min'\n",
    "    file_list = [f for f in os.listdir(root_path) if f.endswith('.csv')]\n",
    "\n",
    "    # sort the file list based on their name\n",
    "    file_list = sorted(file_list, key=lambda x: int(x.split('.')[0]))\n",
    "    flow_cols = ['Time', 'Lane 1 Flow (Veh/h)', 'Lane 2 Flow (Veh/h)', 'Lane 3 Flow (Veh/h)', 'Lane 4 Flow (Veh/h)']\n",
    "    speed_cols = ['Time', 'Lane 1 Speed (km/h)', 'Lane 2 Speed (km/h)', 'Lane 3 Speed (km/h)', 'Lane 4 Speed (km/h)']\n",
    "    occupy_cols = ['Time','Lane 1 Occ (%)','Lane 2 Occ (%)','Lane 3 Occ (%)','Lane 4 Occ (%)']\n",
    "\n",
    "    flow_dfs = []\n",
    "    speed_dfs = []\n",
    "\n",
    "    # read each file one by one\n",
    "    flag = 0\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(root_path, file)\n",
    "        if flag == 1:\n",
    "            flow_cols.remove('Time')\n",
    "            speed_cols.remove('Time')\n",
    "        flow_dfs.append(pd.read_csv(file_path).loc[:, flow_cols])\n",
    "        speed_dfs.append(pd.read_csv(file_path).loc[:, speed_cols])\n",
    "        flag += 1\n",
    "\n",
    "    flow_train_df = pd.concat(flow_dfs,axis=1)\n",
    "    speed_train_df = pd.concat(speed_dfs,axis=1)\n",
    "\n",
    "    flow_train_df.columns = range(len(flow_train_df.columns))\n",
    "    flow_train_df.rename(columns={0:'date'}, inplace=True)\n",
    "    flow_train_df['date'] = datetime_5min_train_ids\n",
    "    flow_train_df['date'] = pd.to_datetime(flow_train_df['date'])\n",
    "    flow_train_df.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "    speed_train_df.columns = range(len(speed_train_df.columns))\n",
    "    speed_train_df.rename(columns={0:'date'}, inplace=True)\n",
    "    speed_train_df['date'] = datetime_5min_train_ids\n",
    "    speed_train_df['date'] = pd.to_datetime(speed_train_df['date'])\n",
    "    speed_train_df.set_index('date', inplace=True)\n",
    "\n",
    "    return flow_train_df, speed_train_df\n",
    "\n",
    "flow_train_df, speed_train_df = load_train_raw(root_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train_arr = flow_train_df.values\n",
    "mask = np.isnan(flow_train_arr)\n",
    "flow_train_arr[mask] = flow_train_impu[mask]\n",
    "flow_train_arr[flow_train_arr < 0] = np.nan\n",
    "\n",
    "speed_train_arr = speed_train_df.values\n",
    "mask = np.isnan(speed_train_arr)\n",
    "speed_train_arr[mask] = speed_train_impu[mask]\n",
    "speed_train_arr[speed_train_arr < 0] = np.nan\n",
    "\n",
    "flow_train_df.iloc[:,:] = flow_train_arr\n",
    "speed_train_df.iloc[:,:] = speed_train_arr\n",
    "\n",
    "flow_train_df.fillna(method='bfill', inplace=True)\n",
    "flow_train_df.fillna(method='ffill', inplace=True)\n",
    "assert np.isnan(flow_train_df.values).sum() == 0\n",
    "flow_train_df.to_csv(root_path+'/flow-5min.csv')\n",
    "\n",
    "speed_train_df.fillna(method='bfill', inplace=True)\n",
    "speed_train_df.fillna(method='ffill', inplace=True)\n",
    "assert np.isnan(speed_train_df.values).sum() == 0\n",
    "speed_train_df.to_csv(root_path+'/speed-5min.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing from the raw data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in flow_df: 0\n",
      "Number of NaN values in speed_df: 0\n"
     ]
    }
   ],
   "source": [
    "datetime_5min_train_ids = pd.date_range(start='2023-04-02', end='2023-07-01', freq='5min', inclusive='both')[:-1]\n",
    "\n",
    "# get a list of all the csv files in the directory\n",
    "root_path = './dataset/competition/train-5min' # './dataset/train-30s'    './dataset/train-5min'\n",
    "file_list = [f for f in os.listdir(root_path) if f.endswith('.csv')]\n",
    "\n",
    "# sort the file list based on their name\n",
    "file_list = sorted(file_list, key=lambda x: int(x.split('.')[0]))\n",
    "flow_cols = ['Time', 'Lane 1 Flow (Veh/h)', 'Lane 2 Flow (Veh/h)', 'Lane 3 Flow (Veh/h)', 'Lane 4 Flow (Veh/h)']\n",
    "speed_cols = ['Time', 'Lane 1 Speed (km/h)', 'Lane 2 Speed (km/h)', 'Lane 3 Speed (km/h)', 'Lane 4 Speed (km/h)']\n",
    "occupy_cols = ['Time','Lane 1 Occ (%)','Lane 2 Occ (%)','Lane 3 Occ (%)','Lane 4 Occ (%)']\n",
    "\n",
    "flow_dfs = []\n",
    "speed_dfs = []\n",
    "occupy_dfs = []\n",
    "\n",
    "# read each file one by one\n",
    "flag = 0\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(root_path, file)\n",
    "    if flag == 1:\n",
    "        flow_cols.remove('Time')\n",
    "        speed_cols.remove('Time')\n",
    "        occupy_cols.remove('Time')\n",
    "    flow_dfs.append(pd.read_csv(file_path).loc[:, flow_cols])\n",
    "    speed_dfs.append(pd.read_csv(file_path).loc[:, speed_cols])\n",
    "    occupy_dfs.append(pd.read_csv(file_path).loc[:, occupy_cols])\n",
    "    flag += 1\n",
    "\n",
    "flow_df = pd.concat(flow_dfs,axis=1)\n",
    "speed_df = pd.concat(speed_dfs,axis=1)\n",
    "occupy_df = pd.concat(occupy_dfs,axis=1)\n",
    "\n",
    "flow_df.columns = range(len(flow_df.columns))\n",
    "flow_df.rename(columns={0:'date'}, inplace=True)\n",
    "flow_df['date'] = datetime_5min_train_ids\n",
    "flow_df['date'] = pd.to_datetime(flow_df['date'])\n",
    "flow_df.set_index('date', inplace=True)\n",
    "flow_df.fillna(method='bfill', inplace=True)\n",
    "flow_df.fillna(method='ffill', inplace=True)\n",
    "flow_df.to_csv(root_path+'/flow-5min.csv')\n",
    "\n",
    "speed_df.columns = range(len(speed_df.columns))\n",
    "speed_df.rename(columns={0:'date'}, inplace=True)\n",
    "speed_df['date'] = datetime_5min_train_ids\n",
    "speed_df['date'] = pd.to_datetime(speed_df['date'])\n",
    "speed_df.set_index('date', inplace=True)\n",
    "speed_df.fillna(method='bfill', inplace=True)\n",
    "speed_df.fillna(method='ffill', inplace=True)\n",
    "speed_df.to_csv(root_path+'/speed-5min.csv')\n",
    "\n",
    "occupy_df.columns = range(len(occupy_df.columns))\n",
    "occupy_df.rename(columns={0:'date'}, inplace=True)\n",
    "occupy_df['date'] = datetime_5min_train_ids\n",
    "occupy_df['date'] = pd.to_datetime(occupy_df['date'])\n",
    "occupy_df.set_index('date', inplace=True)\n",
    "occupy_df.fillna(method='bfill', inplace=True)\n",
    "occupy_df.fillna(method='ffill', inplace=True)\n",
    "occupy_df.to_csv(root_path+'/occupy-5min.csv')\n",
    "\n",
    "print('Number of NaN values in flow_df:', flow_df.isna().sum().sum())\n",
    "print('Number of NaN values in speed_df:', speed_df.isna().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30s Training data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end times of each range\n",
    "ranges_30s = [('00:00:02', '23:59:34')]\n",
    "\n",
    "# Define the start and end dates for the DatetimeIndex\n",
    "start_date = '2023-07-01'\n",
    "end_date = '2023-07-07'\n",
    "\n",
    "# Create an empty list to store the DatetimeIndex values\n",
    "index_values = []\n",
    "\n",
    "# Loop through each date\n",
    "for date in pd.date_range(start=start_date, end=end_date, freq='D'):\n",
    "    # Loop through each range and generate the DatetimeIndex values within that range for the current date\n",
    "    for start_time, end_time in ranges_30s:\n",
    "        index_values += pd.date_range(start=f'{date.date()} {start_time}', end=f'{date.date()} {end_time}', freq='30s', inclusive='both').tolist()\n",
    "\n",
    "# Create the DatetimeIndex from the list of values\n",
    "datetime_30s_train_ids = pd.DatetimeIndex(index_values)\n",
    "\n",
    "datetime_30s_train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_30s_train_ids = pd.date_range(start='2023-04-02 00:00:02', end='2023-07-01 23:59:34', freq='30s', inclusive='both')[:-1]\n",
    "\n",
    "# get a list of all the csv files in the directory\n",
    "root_path = './dataset/train-30s' # './dataset/train-30s'    './dataset/train-5min'\n",
    "file_list = [f for f in os.listdir(root_path) if f.endswith('.csv')]\n",
    "\n",
    "# sort the file list based on their name\n",
    "file_list = sorted(file_list, key=lambda x: int(x.split('.')[0]))\n",
    "flow_cols = ['Time', 'Lane 1 Flow', 'Lane 2 Flow', 'Lane 3 Flow', 'Lane 4 Flow']\n",
    "speed_cols = ['Time', 'Lane 1 G-Factor (22) Speed', 'Lane 2 G-Factor (22) Speed', 'Lane 3 G-Factor (22) Speed', 'Lane 4 G-Factor (22) Speed']\n",
    "occupy_cols = ['Time','Lane 1 Occupancy (%)','Lane 2 Occupancy (%)','Lane 3 Occupancy (%)','Lane 4 Occupancy (%)']\n",
    "\n",
    "flow_dfs = []\n",
    "speed_dfs = []\n",
    "occupy_dfs = []\n",
    "\n",
    "# read each file one by one\n",
    "flag = 0\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(root_path, file)\n",
    "    if flag == 1:\n",
    "        flow_cols.remove('Time')\n",
    "        speed_cols.remove('Time')\n",
    "        occupy_cols.remove('Time')\n",
    "    flow_dfs.append(pd.read_csv(file_path).loc[:, flow_cols])\n",
    "    speed_dfs.append(pd.read_csv(file_path).loc[:, speed_cols])\n",
    "    occupy_dfs.append(pd.read_csv(file_path).loc[:, occupy_cols])\n",
    "    flag += 1\n",
    "\n",
    "flow_df = pd.concat(flow_dfs,axis=1)\n",
    "speed_df = pd.concat(speed_dfs,axis=1)\n",
    "occupy_df = pd.concat(occupy_dfs,axis=1)\n",
    "\n",
    "flow_df.columns = range(len(flow_df.columns))\n",
    "flow_df.rename(columns={0:'date'}, inplace=True)\n",
    "flow_df['date'] = datetime_30s_train_ids\n",
    "flow_df['date'] = pd.to_datetime(flow_df['date'])\n",
    "flow_df.set_index('date', inplace=True)\n",
    "flow_df.fillna(0, inplace=True)\n",
    "flow_df.to_csv(root_path+'/flow-30s.csv')\n",
    "\n",
    "speed_df.columns = range(len(speed_df.columns))\n",
    "speed_df.rename(columns={0:'date'}, inplace=True)\n",
    "speed_df['date'] = datetime_30s_train_ids\n",
    "speed_df['date'] = pd.to_datetime(speed_df['date'])\n",
    "speed_df.set_index('date', inplace=True)\n",
    "speed_df.fillna(0, inplace=True)\n",
    "speed_df.to_csv(root_path+'/speed-30s.csv')\n",
    "\n",
    "occupy_df.columns = range(len(occupy_df.columns))\n",
    "occupy_df.rename(columns={0:'date'}, inplace=True)\n",
    "occupy_df['date'] = datetime_30s_train_ids\n",
    "occupy_df['date'] = pd.to_datetime(occupy_df['date'])\n",
    "occupy_df.set_index('date', inplace=True)\n",
    "occupy_df.fillna(0, inplace=True)\n",
    "occupy_df.to_csv(root_path+'/occupy-30s.csv')\n",
    "\n",
    "print('Number of NaN values in flow_df:', flow_df.isna().sum().sum())\n",
    "print('Number of NaN values in speed_df:', speed_df.isna().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for testing data (method1, not full-day data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in flow_df: 0\n",
      "Number of NaN values in speed_df: 0\n"
     ]
    }
   ],
   "source": [
    "# Define the start and end times of each range\n",
    "ranges_5min = [('05:00:00', '07:55:00'), ('09:30:00', '12:25:00'), ('14:00:00', '16:55:00')]\n",
    "\n",
    "# Define the start and end dates for the DatetimeIndex\n",
    "start_date = '2023-07-01'\n",
    "end_date = '2023-07-07'\n",
    "\n",
    "# Create an empty list to store the DatetimeIndex values\n",
    "index_values = []\n",
    "\n",
    "# Loop through each date\n",
    "for date in pd.date_range(start=start_date, end=end_date, freq='D'):\n",
    "    # Loop through each range and generate the DatetimeIndex values within that range for the current date\n",
    "    for start_time, end_time in ranges_5min:\n",
    "        index_values += pd.date_range(start=f'{date.date()} {start_time}', end=f'{date.date()} {end_time}', freq='5min', inclusive='both').tolist()\n",
    "\n",
    "# Create the DatetimeIndex from the list of values\n",
    "datetime_5min_test_ids = pd.DatetimeIndex(index_values)\n",
    "\n",
    "\n",
    "# ranges_5min_empty = [('05:00:00', '08:55:00'), ('09:30:00', '13:25:00'), ('14:00:00', '17:55:00')]\n",
    "ranges_5min_empty = [('05:00:00', '08:55:00'),('09:30:00', '13:25:00'), ('14:00:00', '17:55:00')]\n",
    "# Create an empty list to store the DatetimeIndex values\n",
    "index_values = []\n",
    "# Loop through each date\n",
    "for date in pd.date_range(start=start_date, end=end_date, freq='D'):\n",
    "    # Loop through each range and generate the DatetimeIndex values within that range for the current date\n",
    "    for start_time, end_time in ranges_5min_empty:\n",
    "        index_values += pd.date_range(start=f'{date.date()} {start_time}', end=f'{date.date()} {end_time}', freq='5min', inclusive='both').tolist()\n",
    "\n",
    "# Create the DatetimeIndex from the list of values\n",
    "empty_df = pd.DatetimeIndex(index_values)\n",
    "empty_df = pd.DataFrame(index=empty_df, columns=[str(i) for i in range(1, 41)])\n",
    "\n",
    "# get a list of all the csv files in the directory\n",
    "root_path = './dataset/competition/test-5min'\n",
    "file_list = [f for f in os.listdir(root_path) if f.endswith('.csv')]\n",
    "\n",
    "# sort the file list based on their name\n",
    "file_list = sorted(file_list, key=lambda x: int(x.split('.')[0]))\n",
    "flow_cols = ['Time', 'Lane 1 Flow (Veh/h)', 'Lane 2 Flow (Veh/h)', 'Lane 3 Flow (Veh/h)', 'Lane 4 Flow (Veh/h)']\n",
    "speed_cols = ['Time', 'Lane 1 Speed (km/h)', 'Lane 2 Speed (km/h)', 'Lane 3 Speed (km/h)', 'Lane 4 Speed (km/h)']\n",
    "\n",
    "flow_dfs = []\n",
    "speed_dfs = []\n",
    "\n",
    "# read each file one by one\n",
    "flag = 0\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(root_path, file)\n",
    "    if flag == 1:\n",
    "        flow_cols.remove('Time')\n",
    "        speed_cols.remove('Time')\n",
    "    flow_dfs.append(pd.read_csv(file_path).loc[:, flow_cols])\n",
    "    speed_dfs.append(pd.read_csv(file_path).loc[:, speed_cols])\n",
    "    flag += 1\n",
    "\n",
    "flow_df = pd.concat(flow_dfs,axis=1)\n",
    "speed_df = pd.concat(speed_dfs,axis=1)\n",
    "\n",
    "flow_df.columns = range(len(flow_df.columns))\n",
    "flow_df.rename(columns={0:'date'}, inplace=True)\n",
    "flow_df['date'] = datetime_5min_test_ids\n",
    "flow_df['date'] = pd.to_datetime(flow_df['date'])\n",
    "flow_df.set_index('date', inplace=True)\n",
    "flow_df.fillna(method='ffill', inplace=True)\n",
    "flow_df.fillna(method='bfill', inplace=True)\n",
    "merged = empty_df.merge(flow_df, how='left', left_index=True, right_index=True)\n",
    "merged.dropna(axis=1, how='all', inplace=True)\n",
    "merged.index.name = 'date'\n",
    "merged.fillna(method='ffill', inplace=True)\n",
    "merged.fillna(method='bfill', inplace=True)\n",
    "merged.to_csv(root_path+'/flow-5min.csv')\n",
    "\n",
    "speed_df.columns = range(len(speed_df.columns))\n",
    "speed_df.rename(columns={0:'date'}, inplace=True)\n",
    "speed_df['date'] = datetime_5min_test_ids\n",
    "speed_df['date'] = pd.to_datetime(speed_df['date'])\n",
    "speed_df.set_index('date', inplace=True)\n",
    "speed_df.fillna(method='ffill', inplace=True)\n",
    "speed_df.fillna(method='bfill', inplace=True)\n",
    "merged = empty_df.merge(speed_df, how='left', left_index=True, right_index=True)\n",
    "merged.dropna(axis=1, how='all', inplace=True)\n",
    "merged.index.name = 'date'\n",
    "merged.fillna(method='ffill', inplace=True)\n",
    "merged.fillna(method='bfill', inplace=True)\n",
    "merged.to_csv(root_path+'/speed-5min.csv')\n",
    "# speed_df.to_csv(root_path+'/speed.csv')\n",
    "\n",
    "print('Number of NaN values in flow_df:', flow_df.isna().sum().sum())\n",
    "print('Number of NaN values in speed_df:', speed_df.isna().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for testing data (method1, full-day data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end times of each range\n",
    "ranges_5min = [('05:00:00', '07:55:00'), ('09:30:00', '12:25:00'), ('14:00:00', '16:55:00')]\n",
    "\n",
    "# Define the start and end dates for the DatetimeIndex\n",
    "start_date = '2023-07-01'\n",
    "end_date = '2023-07-07'\n",
    "\n",
    "# Create an empty list to store the DatetimeIndex values\n",
    "index_values = []\n",
    "\n",
    "# Loop through each date\n",
    "for date in pd.date_range(start=start_date, end=end_date, freq='D'):\n",
    "    # Loop through each range and generate the DatetimeIndex values within that range for the current date\n",
    "    for start_time, end_time in ranges_5min:\n",
    "        index_values += pd.date_range(start=f'{date.date()} {start_time}', end=f'{date.date()} {end_time}', freq='5min', inclusive='both').tolist()\n",
    "\n",
    "# Create the DatetimeIndex from the list of values\n",
    "datetime_5min_test_ids = pd.DatetimeIndex(index_values)\n",
    "\n",
    "# Create the DatetimeIndex from the list of values\n",
    "datetime_5min_test_ids_full = pd.date_range(start=start_date, end='2023-07-08', freq='5min')[:-1]\n",
    "empty_df = pd.DataFrame(index=datetime_5min_test_ids_full, columns=[str(i) for i in range(1, 41)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the csv files in the directory\n",
    "root_path = './dataset/test-5min'\n",
    "file_list = [f for f in os.listdir(root_path) if f.endswith('.csv')]\n",
    "\n",
    "# sort the file list based on their name\n",
    "file_list = sorted(file_list, key=lambda x: int(x.split('.')[0]))\n",
    "flow_cols = ['Time', 'Lane 1 Flow (Veh/h)', 'Lane 2 Flow (Veh/h)', 'Lane 3 Flow (Veh/h)', 'Lane 4 Flow (Veh/h)']\n",
    "speed_cols = ['Time', 'Lane 1 Speed (km/h)', 'Lane 2 Speed (km/h)', 'Lane 3 Speed (km/h)', 'Lane 4 Speed (km/h)']\n",
    "\n",
    "flow_dfs = []\n",
    "speed_dfs = []\n",
    "\n",
    "# read each file one by one\n",
    "flag = 0\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(root_path, file)\n",
    "    if flag == 1:\n",
    "        flow_cols.remove('Time')\n",
    "        speed_cols.remove('Time')\n",
    "    flow_dfs.append(pd.read_csv(file_path).loc[:, flow_cols])\n",
    "    speed_dfs.append(pd.read_csv(file_path).loc[:, speed_cols])\n",
    "    flag += 1\n",
    "\n",
    "flow_df = pd.concat(flow_dfs,axis=1)\n",
    "speed_df = pd.concat(speed_dfs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df.columns = range(len(flow_df.columns))\n",
    "flow_df.rename(columns={0:'date'}, inplace=True)\n",
    "flow_df['date'] = datetime_5min_test_ids\n",
    "flow_df['date'] = pd.to_datetime(flow_df['date'])\n",
    "flow_df.set_index('date', inplace=True)\n",
    "\n",
    "merged = empty_df.merge(flow_df, how='left', left_index=True, right_index=True)\n",
    "merged.dropna(axis=1, how='all', inplace=True)\n",
    "merged.index.name = 'date'\n",
    "merged.fillna(0, inplace=True)\n",
    "merged.to_csv(root_path+'/flow.csv')\n",
    "\n",
    "speed_df.columns = range(len(speed_df.columns))\n",
    "speed_df.rename(columns={0:'date'}, inplace=True)\n",
    "speed_df['date'] = datetime_5min_test_ids\n",
    "speed_df['date'] = pd.to_datetime(speed_df['date'])\n",
    "speed_df.set_index('date', inplace=True)\n",
    "\n",
    "merged = empty_df.merge(speed_df, how='left', left_index=True, right_index=True)\n",
    "merged.dropna(axis=1, how='all', inplace=True)\n",
    "merged.index.name = 'date'\n",
    "merged.fillna(0, inplace=True)\n",
    "merged.to_csv(root_path+'/speed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch131",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
